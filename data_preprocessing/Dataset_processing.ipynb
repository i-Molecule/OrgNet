{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ADD FULL (not relative) paths in the ###### areas, e.g. \n",
    "# CORRECT: ROSETTA_BIN = \"/home/user/.../rosetta/main/source/bin\"\n",
    "# INCORRECT:  ROSETTA_BIN = \"rosetta/main/source/bin\"\n",
    "\n",
    "# NOTE: if you followed README, this notebook should run as is, no modifications required.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import shutil\n",
    "import Bio\n",
    "from Bio import SeqUtils\n",
    "from Bio.PDB import PDBParser, PDBIO\n",
    "from Bio.SeqUtils import seq1\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "import logging\n",
    "\n",
    "# Get the absolute path of the current working directory\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Constants\n",
    "###########################################################################################################################################\n",
    "N_JOBS = 8\n",
    "RUN_FAST_RELAX = True\n",
    "######## FULL PATHS ARE OBLIGATORY ##############\n",
    "# ROSETTA_BIN = \"absolute/path/rosetta/main/source/bin\" \n",
    "ROSETTA_BIN = os.path.join(notebook_dir, \"rosetta/main/source/bin\")\n",
    "# ROSETTA_PATH = \"absolute/path/rosetta/main/source/bin/relax.static.linuxgccrelease\"\n",
    "ROSETTA_PATH = os.path.join(notebook_dir, \"rosetta/main/source/bin/relax.static.linuxgccrelease\")\n",
    "flag_parallel = True\n",
    "###########################################################################################################################################\n",
    "for path in [ROSETTA_BIN, ROSETTA_PATH]:\n",
    "    assert os.path.exists(path), f\"Path does not exist: {path}\"\n",
    "    assert os.path.isabs(path), f\"Path is not absolute: {path}\"\n",
    "\n",
    "\n",
    "# Paths\n",
    "###########################################################################################################################################\n",
    "# path_to_save_ds = \"absolute/path/to/output/directory\"\n",
    "path_to_save_ds = notebook_dir\n",
    "dataset_name = \"Ssym\"\n",
    "###########################################################################################################################################\n",
    "path_to_ds = os.path.join(path_to_save_ds, dataset_name)\n",
    "path_to_dw_add_pdbs = os.path.join(path_to_ds, \"pdbs\")\n",
    "path_to_chains = os.path.join(path_to_ds, \"chains\")\n",
    "path_to_pdb_chains_filtered = os.path.join(path_to_ds, \"pdbs_chains_filtered\")\n",
    "path_to_save_rem_hetatm = os.path.join(path_to_ds, \"pdb_chains_hetatm_rem\")\n",
    "path_to_relaxed_chains = os.path.join(path_to_ds, \"relaxed_chains\")\n",
    "relaxed_chains_total = os.path.join(path_to_ds, \"relaxed_chains_total\")\n",
    "relaxed_chains_total_ori = os.path.join(path_to_ds, \"relaxed_chains_total_ori\")\n",
    "rosetta_out = os.path.join(path_to_ds, \"rosetta_out\")\n",
    "features = os.path.join(path_to_ds, \"features\")\n",
    "features_ds = os.path.join(features, f\"{dataset_name}_ori\")\n",
    "features_ds_nonori = os.path.join(features, f\"{dataset_name}_nonori\")\n",
    "ds = os.path.join(path_to_ds, \"ds\") # NOTE: this is the final datasets directory\n",
    "\n",
    "###########################################################################################################################################\n",
    "path_to_ori_script = os.path.join(notebook_dir, \"orientation_standardization/orient_protein.py\")\n",
    "# path_to_ft_calc_script = os.path.join(notebook_dir, \"calculate_features_for_thermonet.py\") #no GLY correction (Thermonet)\n",
    "path_to_ft_calc_script = os.path.join(notebook_dir, \"calculate_features_for_orgnet.py\") #with GLY correction (Orgnet)\n",
    "###########################################################################################################################################\n",
    "for path in [path_to_ori_script, path_to_ft_calc_script]:\n",
    "    assert os.path.exists(path), f\"Path does not exist: {path}\"\n",
    "    assert os.path.isabs(path), f\"Path is not absolute: {path}\"\n",
    "\n",
    "log = os.path.join(path_to_ds, \"ori_log.txt\")\n",
    "os.environ['HTMD_NONINTERACTIVE'] = '1'\n",
    "\n",
    "# Configure logging\n",
    "log_file = os.path.join(path_to_save_ds, dataset_name, \"process.log\")\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "def make_dirs():\n",
    "    \"\"\"\n",
    "    Create the required directory structure for dataset processing.\n",
    "    \n",
    "    - Creates main directories (dataset, pdbs, chains, features, etc.).\n",
    "    - Also creates subdirectories under the features directory for different feature types.\n",
    "    \"\"\"\n",
    "    directories = [\n",
    "        path_to_ds,\n",
    "        path_to_dw_add_pdbs,\n",
    "        path_to_chains,\n",
    "        path_to_pdb_chains_filtered,\n",
    "        path_to_save_rem_hetatm,\n",
    "        path_to_relaxed_chains,\n",
    "        relaxed_chains_total,\n",
    "        relaxed_chains_total_ori,\n",
    "        rosetta_out,\n",
    "        features,\n",
    "        features_ds,\n",
    "        features_ds_nonori,\n",
    "        ds\n",
    "    ]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "    sub_dirs = [\"defdif_direct\", \"def_direct\", \"dif_direct\",\n",
    "                \"defdif_reverse\", \"def_reverse\", \"dif_reverse\"]\n",
    "    for item in sub_dirs:\n",
    "        os.makedirs(os.path.join(features_ds, f\"{dataset_name}_{item}\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(features_ds_nonori, f\"{dataset_name}_{item}\"), exist_ok=True)\n",
    "\n",
    "def make_chain_dirs(path, chains_list):\n",
    "    \"\"\"\n",
    "    Create directories for individual protein chains.\n",
    "    \n",
    "    Parameters:\n",
    "    - path (str): Base directory where chain directories will be created.\n",
    "    - chains_list (list): List of chain identifiers.\n",
    "    \"\"\"\n",
    "    for chain in chains_list:\n",
    "        os.makedirs(os.path.join(path, chain), exist_ok=True)\n",
    "    \n",
    "\n",
    "def download_pdb(pdb, path_to_dwl):\n",
    "    \"\"\"\n",
    "    Download a single PDB file from the RCSB repository.\n",
    "    \n",
    "    Parameters:\n",
    "    - pdb (str): The PDB id to download.\n",
    "    - path_to_dwl (str): Directory where the downloaded PDB file will be saved.\n",
    "    \n",
    "    Operation:\n",
    "    - Constructs the file name and URL.\n",
    "    - Downloads the file using requests.\n",
    "    - Writes the file if the download is successful.\n",
    "    \"\"\"\n",
    "    fname = os.path.join(path_to_dwl, f'{pdb}.pdb')\n",
    "    url = f'https://files.rcsb.org/download/{pdb}.pdb'\n",
    "    v = requests.get(url)\n",
    "    if v.status_code != 200:\n",
    "        logging.warning(f\"{url} status code {v.status_code}\")\n",
    "        return\n",
    "    with open(fname, 'w+') as f:\n",
    "        f.write(v.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "def download_pdbs_list(lst, path_to_dwl):\n",
    "    \"\"\"\n",
    "    Download multiple PDB files concurrently.\n",
    "    \n",
    "    Parameters:\n",
    "    - lst (list): List of PDB ids to download.\n",
    "    - path_to_dwl (str): Directory where the downloaded PDB files will be stored.\n",
    "    \n",
    "    Operation:\n",
    "    - Determines which PDBs are not already downloaded.\n",
    "    - Uses ThreadPoolExecutor to download the missing PDB files in parallel.\n",
    "    - Displays progress via tqdm.\n",
    "    \"\"\"\n",
    "    existing_pdbs = set([os.path.basename(f)[:4] for f in glob.glob(os.path.join(path_to_dwl, '*.pdb'))])\n",
    "    to_download = set(lst) - existing_pdbs\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=N_JOBS) as executor:\n",
    "        list(tqdm(executor.map(lambda pdb: download_pdb(pdb, path_to_dwl), to_download), total=len(to_download), desc='Downloading PDB files'))\n",
    "\n",
    "\n",
    "def pdbs_to_chains(path_to_pdbs, path_to_chains):\n",
    "    \"\"\"\n",
    "    Process PDB files to extract individual chains and associated sequence information.\n",
    "    \n",
    "    Parameters:\n",
    "    - path_to_pdbs (str): Directory containing the PDB files.\n",
    "    - path_to_chains (str): Directory where individual chain PDB files will be saved.\n",
    "    \n",
    "    Operation:\n",
    "    - Parses each PDB file using BioPython's PDBParser.\n",
    "    - Validates that all models in a structure have the same sequence.\n",
    "    - For each chain, extracts the sequence and residue IDs.\n",
    "    - Saves each chain to a separate PDB file.\n",
    "    - Returns a Pandas DataFrame summarizing the chain metadata.\n",
    "    \"\"\"\n",
    "    chain_seq = []\n",
    "    parser = PDBParser()\n",
    "    io = PDBIO()\n",
    "    pdb_files = glob.glob(os.path.join(path_to_pdbs, '*.pdb'))\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for pdb in tqdm(pdb_files, desc='Processing PDBs'):\n",
    "            pdb_id = os.path.basename(pdb)[:4]\n",
    "            structure = parser.get_structure(pdb_id, pdb)\n",
    "\n",
    "            # Validate models\n",
    "            models = list(structure.get_models())\n",
    "            for i in range(len(models) - 1):\n",
    "                s1 = seq1(''.join(residue.resname for residue in models[i].get_residues()))\n",
    "                s2 = seq1(''.join(residue.resname for residue in models[i + 1].get_residues()))\n",
    "                assert s1 == s2, \"Invalid models\"\n",
    "\n",
    "            pdb_chains = structure.get_chains()\n",
    "\n",
    "            for chain in pdb_chains:\n",
    "                chain_name = f'{structure.get_id()}{chain.get_id()}'\n",
    "                chain_seq.append({\n",
    "                    'PDB_chain': chain_name,\n",
    "                    'sequence': seq1(''.join(residue.resname for residue in chain)),\n",
    "                    'pdb_ids': tuple([r.get_id()[1] for r in chain]),\n",
    "                })\n",
    "                chain_file = os.path.join(path_to_chains, f'{chain_name}.pdb')\n",
    "                if not os.path.exists(chain_file):\n",
    "                    io.set_structure(chain)\n",
    "                    io.save(chain_file)\n",
    "\n",
    "    return pd.DataFrame(chain_seq).drop_duplicates()\n",
    "    \n",
    "\n",
    "def filter_chains(path_to_chains, path_to_pdb_chains_filtered):\n",
    "    \"\"\"\n",
    "    Filter and copy chains based on an unwanted chain set.\n",
    "    \n",
    "    Parameters:\n",
    "    - path_to_chains (str): Directory containing individual chain PDB files.\n",
    "    - path_to_pdb_chains_filtered (str): Destination directory for filtered chains.\n",
    "    \n",
    "    Operation:\n",
    "    - Uses a set of unwanted chain identifiers (`un_pdb_chain_dat` must be defined externally).\n",
    "    - Copies chain files whose id is in the unwanted set to the filtered directory.\n",
    "    \"\"\"\n",
    "    un_pdb_chain_dat_set = set(un_pdb_chain_dat)  # Convert to set for faster lookup\n",
    "    for filename in tqdm(os.listdir(path_to_chains), desc='Filtering Chains'):\n",
    "        chain_id = filename[:5]\n",
    "        if chain_id in un_pdb_chain_dat_set:\n",
    "            pdb_chain_path = os.path.join(path_to_chains, filename)\n",
    "            shutil.copy(pdb_chain_path, path_to_pdb_chains_filtered)\n",
    "    \n",
    "\n",
    "def rem_hetatm(path_to_flt_pdbs, path_to_save_rem_hetatm):\n",
    "    \"\"\"\n",
    "    Remove heteroatom records from PDB files.\n",
    "    \n",
    "    Parameters:\n",
    "    - path_to_flt_pdbs (str): Directory containing filtered PDB files.\n",
    "    - path_to_save_rem_hetatm (str): Destination directory to save PDB files without HETATM lines.\n",
    "    \n",
    "    Operation:\n",
    "    - Reads each file line-by-line and writes out lines that do not start with 'HETATM'.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(path_to_flt_pdbs):\n",
    "        input_file = os.path.join(path_to_flt_pdbs, file)\n",
    "        output_file = os.path.join(path_to_save_rem_hetatm, file)\n",
    "        with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "            for line in infile:\n",
    "                if not line.startswith('HETATM'):\n",
    "                    outfile.write(line)\n",
    "                    \n",
    "\n",
    "def run_relax():\n",
    "    \"\"\"\n",
    "    Run the Rosetta relax protocol on non-relaxed PDB chains.\n",
    "    \n",
    "    Operation:\n",
    "    - If the RUN_FAST_RELAX flag is set, constructs a command to relax the PDB chains using Rosetta.\n",
    "    - Uses GNU parallel to execute the relax command on multiple PDB files.\n",
    "    - Logs the command and output to a log file.\n",
    "    \"\"\"\n",
    "    print(\"RUNNING WT ROSETTA\")\n",
    "    if RUN_FAST_RELAX:\n",
    "        nrc = ' '.join([f'{f}.pdb' for f in non_relaxed_chains])\n",
    "        cmd = f\"cd {path_to_save_rem_hetatm} && ls {nrc} | parallel -j {N_JOBS} {ROSETTA_BIN}/relax.static.linuxgccrelease -in:file:s {{}}  -relax:constrain_relax_to_start_coords -out:suffix _relaxed -out:no_nstruct_label -relax:ramp_constraints false\"\n",
    "        logging.info(f\"Executing: {cmd}\")\n",
    "        log_file_path = os.path.join(path_to_ds, 'rosetta_relax.log')\n",
    "        with open(log_file_path, 'w') as logfile:\n",
    "            subprocess.run(cmd, shell=True, check=True, stdout=logfile, stderr=subprocess.STDOUT)\n",
    "    \n",
    "\n",
    "def copy_relaxed(path_to_dat):\n",
    "    \"\"\"\n",
    "    Copy relaxed PDB chains to a designated folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - path_to_dat (str): Base directory containing the relaxed chains.\n",
    "    \n",
    "    Operation:\n",
    "    - Removes any pre-existing score file if present.\n",
    "    - Iterates over relaxed chain files and copies those containing \"relaxed\" in their filename into specific subdirectories.\n",
    "    \"\"\"\n",
    "    score_file = os.path.join(path_to_dat, \"pdb_chains_hetatm_rem\", \"score_relaxed.sc\")\n",
    "    if os.path.exists(score_file):\n",
    "        os.remove(score_file)\n",
    "        logging.info(f\"Removed {score_file}\")\n",
    "        \n",
    "    relaxed_dir = os.path.join(path_to_dat, \"pdb_chains_hetatm_rem\")\n",
    "    for filename in tqdm(os.listdir(relaxed_dir), desc='Copying Relaxed Chains'):\n",
    "        if \"relaxed\" in filename:\n",
    "            rel_ch = os.path.join(relaxed_dir, filename)\n",
    "            ch_id = filename[:5]\n",
    "            path_to_copy = os.path.join(path_to_dat, \"relaxed_chains\", ch_id)\n",
    "            os.makedirs(path_to_copy, exist_ok=True)\n",
    "            shutil.copy(rel_ch, path_to_copy)\n",
    "            \n",
    "\n",
    "def create_mut_df():\n",
    "    \"\"\"\n",
    "    Create a DataFrame of mutation data.\n",
    "    \n",
    "    Operation:\n",
    "    - Extracts 'pdb', 'pos', 'wt', and 'mut' columns from the global DataFrame `init_df`.\n",
    "    - Renames the columns to uppercase for consistency.\n",
    "    - Returns the new mutation DataFrame.\n",
    "    \"\"\"\n",
    "    ds_mut = init_df[['pdb', 'pos', 'wt', 'mut']].copy()\n",
    "    ds_mut.columns = ['PDB', 'POS', 'WT', 'MUT']\n",
    "    return ds_mut\n",
    "    \n",
    "\n",
    "def run_rosetta_relax(pdb_id, wt, mut, pos, path_to_relaxed_chains, PDBDIR, OUTDIR, ROSETTA_PATH):\n",
    "    \"\"\"\n",
    "    Run Rosetta relax protocol for a specific mutation.\n",
    "    \n",
    "    Parameters:\n",
    "    - pdb_id (str): PDB identifier.\n",
    "    - wt (str): Wild-type amino acid.\n",
    "    - mut (str): Mutant amino acid.\n",
    "    - pos (str or int): Position of the mutation.\n",
    "    - path_to_relaxed_chains (str): Directory with relaxed chain files.\n",
    "    - PDBDIR (str): Not used directly here, but expected for input structure directory.\n",
    "    - OUTDIR (str): Output directory for Rosetta relax results.\n",
    "    - ROSETTA_PATH (str): Path to the Rosetta relax binary.\n",
    "    \n",
    "    Operation:\n",
    "    - Creates an output folder and a Rosetta resfile specifying the mutation.\n",
    "    - Constructs and runs the Rosetta command.\n",
    "    - Logs the command and results.\n",
    "    - Raises an error if mutation data is invalid.\n",
    "    \"\"\"\n",
    "    out_folder = os.path.join(OUTDIR, pdb_id)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "        \n",
    "    if wt != '-' and mut != '-' and pos != '-':\n",
    "        variant = wt + str(pos) + mut  # e.g., D234K\n",
    "\n",
    "        # Create a resfile\n",
    "        variant_resfile = os.path.join(out_folder, f'{pdb_id}_{variant}.resfile')\n",
    "        with open(variant_resfile, 'wt') as opf:\n",
    "            opf.write('NATAA\\n')\n",
    "            opf.write('start\\n')\n",
    "            opf.write(f\"{variant[1:-1]} {pdb_id[-1]} PIKAA {variant[-1]}\\n\")\n",
    "\n",
    "        # Rosetta command\n",
    "        start_struct = os.path.join(path_to_relaxed_chains, f'{pdb_id}/{pdb_id}_relaxed.pdb')\n",
    "        if not os.path.exists(start_struct):\n",
    "            logging.error(f\"No such file {start_struct}\")\n",
    "            return \n",
    "        \n",
    "        cmd = [\n",
    "            ROSETTA_PATH, \n",
    "            '-in:file:s', start_struct, '-in:file:fullatom',\n",
    "            '-relax:constrain_relax_to_start_coords',\n",
    "            '-out:no_nstruct_label', '-relax:ramp_constraints', 'false',\n",
    "            '-relax:respect_resfile',\n",
    "            '-packing:resfile', variant_resfile,\n",
    "            '-default_max_cycles', '200',\n",
    "            '-out:file:scorefile', os.path.join(out_folder, f'{pdb_id}_{variant}_relaxed.sc'),\n",
    "            '-out:suffix', f'_{variant}_relaxed'\n",
    "        ]\n",
    "\n",
    "        cmd_str = ' '.join(cmd)\n",
    "        logging.info(f\"Executing: {cmd_str}\")\n",
    "        log_file_path = os.path.join(out_folder, f'{pdb_id}_{variant}_relax.log')\n",
    "        with open(log_file_path, 'w') as logfile:\n",
    "            subprocess.run(cmd, cwd=out_folder, stdout=logfile, stderr=subprocess.STDOUT)\n",
    "\n",
    "        # Move the output PDB file\n",
    "        output_pdb = os.path.join(out_folder, f\"{pdb_id}_relaxed_{variant}_relaxed.pdb\")\n",
    "        if os.path.exists(output_pdb):\n",
    "            logging.info(f\"Output PDB file found: {output_pdb}\")\n",
    "            # The file is already in the correct folder\n",
    "        else:\n",
    "            logging.error(f\"Output PDB file not found: {output_pdb}\")\n",
    "    else:\n",
    "        logging.error(\"Invalid mutation data\")\n",
    "        raise ValueError('Not clear if this row is for wildtype or a mutant type')\n",
    "    \n",
    "\n",
    "def run_rosetta_for_mutants():\n",
    "    \"\"\"\n",
    "    Process multiple mutations in parallel using Rosetta relax protocol.\n",
    "    \n",
    "    Operation:\n",
    "    - Filters mutation entries in the global mutation DataFrame (`ds_mut`) for valid mutants.\n",
    "    - Determines CPU availability and creates a pool for parallel processing.\n",
    "    - Constructs an argument list and executes `run_rosetta_relax` in parallel using multiprocessing.\n",
    "    \"\"\"\n",
    "    print(\"RUNNING ROSETTA\")\n",
    "    if flag_parallel:\n",
    "        df = ds_mut.copy()\n",
    "        df = df[(df['WT'] != '-') & (df['MUT'] != '-')]\n",
    "        df.dropna(inplace=True)\n",
    "        df['POS'] = df['POS'].astype(int)\n",
    "\n",
    "        n_cpu = mp.cpu_count()\n",
    "        pool_size = max(1, n_cpu - 1)\n",
    "        logging.info(f'Using {pool_size} CPUs')\n",
    "\n",
    "        args_list = [(row['PDB'], row['WT'], row['MUT'], row['POS'],\n",
    "                      path_to_relaxed_chains, path_to_relaxed_chains,\n",
    "                      rosetta_out, ROSETTA_PATH) for _, row in df.iterrows()]\n",
    "\n",
    "        with mp.Pool(pool_size) as pool:\n",
    "            list(tqdm(pool.starmap(run_rosetta_relax, args_list), total=len(args_list)))\n",
    "\n",
    "\n",
    "def copy_pdb_files_and_directories(src_folders, dest_folder):\n",
    "    \"\"\"\n",
    "    Recursively copy PDB files from source folders to a destination folder while preserving directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - src_folders (list): List of source directories to search for PDB files.\n",
    "    - dest_folder (str): Destination directory where PDB files will be copied.\n",
    "    \n",
    "    Operation:\n",
    "    - Walks through each source folder and copies files ending with \".pdb\" to the corresponding relative path in the destination.\n",
    "    \"\"\"\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "    for src_folder in src_folders:\n",
    "        for root, dirs, files in os.walk(src_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pdb\"):\n",
    "                    relative_path = os.path.relpath(root, src_folder)\n",
    "                    dest_dir = os.path.join(dest_folder, relative_path)\n",
    "                    os.makedirs(dest_dir, exist_ok=True)\n",
    "                    shutil.copy2(os.path.join(root, file), os.path.join(dest_dir, file))\n",
    "\n",
    "def feature_calc_nonori(ptc, ptofeat):\n",
    "    \"\"\"\n",
    "    Calculate nonoriented voxel-based features for protein structures.\n",
    "    \n",
    "    Parameters:\n",
    "    - ptc (str): Path to the directory containing protein structure folders.\n",
    "    - ptofeat (str): Base directory where feature subdirectories are located.\n",
    "    \n",
    "    Operation:\n",
    "    - For each protein folder, creates necessary subdirectories.\n",
    "    - Identifies mutant PDB files by a naming pattern.\n",
    "    - For each mutant, extracts the mutation position and constructs paths for both mutant and wild type PDBs.\n",
    "    - Executes an external feature calculation script via subprocess.\n",
    "    - Logs warnings for missing files and errors for failed folders.\n",
    "    \"\"\"\n",
    "    bad_fold = []\n",
    "    log_file_path = os.path.join(path_to_ds, 'feature_calc_nonori.log')\n",
    "    for folder in tqdm(os.listdir(ptc), desc='Calculating nonoriented features'):\n",
    "        try:\n",
    "            for ff in os.listdir(ptofeat):\n",
    "                os.makedirs(os.path.join(ptofeat, ff, folder), exist_ok=True)\n",
    "\n",
    "            folder_path = os.path.join(ptc, folder+\"/\")\n",
    "            mut_prots = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if \"_relaxed_\" in f]\n",
    "            \n",
    "            # Process each mutant protein file\n",
    "            for pdb_file in mut_prots:\n",
    "                pos = pdb_file.split(\"/\")[-1].split(\"_\")[2][1:-1]\n",
    "                path_to_wt = os.path.join(folder_path, f\"{folder}_relaxed.pdb\")\n",
    "                path_to_mut = pdb_file\n",
    "                if os.path.exists(path_to_mut) and os.path.exists(path_to_wt):\n",
    "                    cmd = f\"python {path_to_ft_calc_script} -iwt {path_to_wt} -imut {path_to_mut} -o {ptofeat+'/'} --boxsize 16 --voxelsize 1\"\n",
    "                    with open(log_file_path, 'a') as logfile:\n",
    "                        subprocess.run(cmd, shell=True, stdout=logfile, stderr=subprocess.STDOUT)\n",
    "                else:\n",
    "                    logging.warning(f\"Files not found: {path_to_mut}, {path_to_wt}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing folder {folder}: {e}\")\n",
    "            bad_fold.append(folder)\n",
    "\n",
    "def orient_dataset(wk_dir, out_dir):\n",
    "    \"\"\"\n",
    "    Orient protein structures using standardized orientation.\n",
    "    \n",
    "    Parameters:\n",
    "    - wk_dir (str): Working directory containing folders of protein structures after Rosetta relax protocol.\n",
    "    - out_dir (str): Directory where oriented structures will be saved.\n",
    "    \n",
    "    Operation:\n",
    "    - For each folder, identifies a reference PDB file based on a naming convention.\n",
    "    - Constructs and executes two commands per mutant file:\n",
    "      one for the mutant structure and one for the wild type using the reference file.\n",
    "    - Executes the orientation commands via subprocess and logs the output.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for folder in tqdm(os.listdir(wk_dir), desc='Orienting Dataset'):\n",
    "        folder_path = os.path.join(wk_dir, folder+\"/\")\n",
    "        out_folder = os.path.join(out_dir, folder+\"/\")\n",
    "\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "        ref_pdb_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdb\") and len(f.split(\"_\")) == 2]\n",
    "        if not ref_pdb_files:\n",
    "            continue\n",
    "        ref_pdb_file = ref_pdb_files[0]\n",
    "\n",
    "        pdb_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdb\") and f != ref_pdb_file]\n",
    "        if not pdb_files:\n",
    "            continue\n",
    "\n",
    "        for pdb_file in pdb_files:\n",
    "            pdb_file_path = os.path.join(folder_path, pdb_file)\n",
    "            pos = pdb_file.split(\"_\")[-2][1:-1]\n",
    "            cmd1 = f'python {path_to_ori_script} -i {pdb_file_path} -o {out_folder} --mut_pos {pos} -fl 0'\n",
    "            cmd2 = f'python {path_to_ori_script} -i {os.path.join(folder_path, ref_pdb_file)} -o {out_folder} --mut_pos {pos} -fl {pos}_wt'\n",
    "\n",
    "            with open(log, 'a') as logfile:\n",
    "                subprocess.run(cmd1, shell=True, stdout=logfile, stderr=subprocess.STDOUT)\n",
    "                subprocess.run(cmd2, shell=True, stdout=logfile, stderr=subprocess.STDOUT)\n",
    "                \n",
    "\n",
    "def feature_calc(ptc, ptofeat):\n",
    "    \"\"\"\n",
    "    Calculate oriented voxel-based features for protein structures.\n",
    "    \n",
    "    Parameters:\n",
    "    - ptc (str): Path to the directory containing protein structure folders.\n",
    "    - ptofeat (str): Base directory where feature subdirectories are located.\n",
    "    \n",
    "    Operation:\n",
    "    - For each protein folder, creates necessary subdirectories.\n",
    "    - Identifies mutant PDB files by a naming pattern.\n",
    "    - For each mutant, extracts the mutation position and constructs paths for both mutant and wild type PDBs.\n",
    "    - Executes an external feature calculation script via subprocess.\n",
    "    - Logs warnings for missing files and errors for failed folders.\n",
    "    \"\"\"\n",
    "    bad_fold = []\n",
    "    log_file_path = os.path.join(path_to_ds, 'feature_calc_ori.log')\n",
    "    for folder in tqdm(os.listdir(ptc), desc='Calculating features'):\n",
    "        try:\n",
    "            for ff in os.listdir(ptofeat):\n",
    "                os.makedirs(os.path.join(ptofeat, ff, folder), exist_ok=True)\n",
    "\n",
    "            folder_path = os.path.join(ptc, folder+\"/\")\n",
    "            mut_prots = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if \"_relaxed_0_oriented.pdb\" in f]\n",
    "            \n",
    "            # Process each mutant protein file\n",
    "            for pdb_file in mut_prots:\n",
    "                pos = pdb_file.split(\"/\")[-1].split(\"_\")[2][1:-1]\n",
    "                path_to_wt = os.path.join(folder_path, f\"{folder}_relaxed_{pos}_wt_oriented.pdb\")\n",
    "                path_to_mut = pdb_file\n",
    "                if os.path.exists(path_to_mut) and os.path.exists(path_to_wt):\n",
    "                    cmd = f\"python {path_to_ft_calc_script} -iwt {path_to_wt} -imut {path_to_mut} -o {ptofeat+'/'} --boxsize 16 --voxelsize 1\"\n",
    "                    with open(log_file_path, 'a') as logfile:\n",
    "                        subprocess.run(cmd, shell=True, stdout=logfile, stderr=subprocess.STDOUT)\n",
    "                else:\n",
    "                    logging.warning(f\"Files not found: {path_to_mut}, {path_to_wt}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing folder {folder}: {e}\")\n",
    "            bad_fold.append(folder)\n",
    "                \n",
    "\n",
    "def load_dataset_dir(evaluation_dataset_path, evaluation_features_dir_dir):\n",
    "    \"\"\"\n",
    "    Load and process the dataset for direct mutations.\n",
    "    \n",
    "    Parameters:\n",
    "    - evaluation_dataset_path (str): Path to the CSV file containing mutation data.\n",
    "    - evaluation_features_dir_dir (str): Directory where direct mutation feature files are stored.\n",
    "    \n",
    "    Operation:\n",
    "    - Reads the CSV file and logs the total number of unique mutations.\n",
    "    - Constructs file paths for features and filters out rows where features are missing.\n",
    "    - Loads the features (NumPy arrays) and transposes them for further use.\n",
    "    - Returns the processed DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading dataset direct mutations\")\n",
    "    df = pd.read_csv(evaluation_dataset_path)\n",
    "    logging.info(f'Total unique mutations: {len(df)}')\n",
    "\n",
    "    df['features'] = df.apply(lambda r: os.path.join(evaluation_features_dir_dir, f\"{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy\"), axis=1)\n",
    "    df = df[df.features.apply(os.path.exists)]\n",
    "    logging.info(f'Total mutations with features: {len(df)}')\n",
    "    df['features'] = [np.load(f) for f in tqdm(df.features, desc=\"Loading features\")]\n",
    "    logging.info(f'Total mutations after filtering: {len(df)}')\n",
    "\n",
    "    df.features = df.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    return df\n",
    "    \n",
    "\n",
    "def load_dataset_rev(evaluation_dataset_path, evaluation_features_dir_rev):\n",
    "    \"\"\"\n",
    "    Load and process the dataset for reverse mutations.\n",
    "    \n",
    "    Parameters:\n",
    "    - evaluation_dataset_path (str): Path to the CSV file containing mutation data.\n",
    "    - evaluation_features_dir_rev (str): Directory where reverse mutation feature files are stored.\n",
    "    \n",
    "    Operation:\n",
    "    - Reads the CSV file and inverts ΔΔG values.\n",
    "    - Constructs file paths for features and filters rows without existing feature files.\n",
    "    - Loads and transposes the features.\n",
    "    - Returns the processed DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info('Loading dataset reverse mutations')\n",
    "    df_rev = pd.read_csv(evaluation_dataset_path)\n",
    "    df_rev.ddg = -df_rev.ddg\n",
    "\n",
    "    df_rev['features'] = df_rev.apply(lambda r: os.path.join(evaluation_features_dir_rev, f\"{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy\"), axis=1)\n",
    "    df_rev = df_rev[df_rev.features.apply(os.path.exists)]\n",
    "    logging.info(f'Total mutations with features: {len(df_rev)}')\n",
    "    df_rev['features'] = [np.load(f) for f in tqdm(df_rev.features, desc=\"Loading features\")]\n",
    "    logging.info(f'Total mutations after filtering: {len(df_rev)}')\n",
    "\n",
    "    df_rev.features = df_rev.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    return df_rev\n",
    "    \n",
    "\n",
    "def save_dataset_npy():\n",
    "    \"\"\"\n",
    "    Save the processed features and ΔΔG values for both direct and reverse mutation datasets as NumPy files.\n",
    "    \n",
    "    Operation:\n",
    "    - Loads the direct and reverse datasets using the respective functions.\n",
    "    - Extracts features and ΔΔG values.\n",
    "    - Saves these arrays to .npy files in the specified dataset directory.\n",
    "    \"\"\"\n",
    "    df_train_dataset_dir = load_dataset_dir(evaluation_dataset_path, evaluation_features_dir_dir)\n",
    "    df_train_dataset_rev = load_dataset_rev(evaluation_dataset_path, evaluation_features_dir_rev)\n",
    "\n",
    "    X_direct_dataset_dir = np.array(df_train_dataset_dir.features.to_list())\n",
    "    y_direct_dataset_dir = df_train_dataset_dir.ddg.to_numpy()\n",
    "\n",
    "    X_direct_dataset_rev = np.array(df_train_dataset_rev.features.to_list())\n",
    "    y_direct_dataset_rev = df_train_dataset_rev.ddg.to_numpy()\n",
    "\n",
    "    np.save(os.path.join(ds+\"/\", f\"{dataset_name}_X_direct.npy\"), X_direct_dataset_dir)\n",
    "    np.save(os.path.join(ds+\"/\", f\"{dataset_name}_y_direct.npy\"), y_direct_dataset_dir)\n",
    "    np.save(os.path.join(ds+\"/\", f\"{dataset_name}_X_reverse.npy\"), X_direct_dataset_rev)\n",
    "    np.save(os.path.join(ds+\"/\", f\"{dataset_name}_y_reverse.npy\"), y_direct_dataset_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input df\n",
    "# init_df = pd.read_csv(\"/datasets/path_to_your_csv_dataset/\")  # NOTE: columns in dataset - \"pdb\", \"pos\", \"wt\", \"mut\", \"ddg\"\n",
    "init_df = pd.read_csv(\"datasets/Ssym.csv\").rename(columns={\n",
    "    \"pdb_id\": \"pdb\",\n",
    "    \"position\": \"pos\",\n",
    "    \"wild_type\": \"wt\",\n",
    "    \"mutant\": \"mut\",\n",
    "    \"ddg\": \"ddg\"\n",
    "})\n",
    "init_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04251960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the following commands\n",
    "make_dirs()\n",
    "\n",
    "un_pdb_dat = list(set([f[0:4] for f in init_df[\"pdb\"].unique().tolist() if \"AF-\" not in f]))\n",
    "#print(len(un_pdb_dat), un_pdb_dat)\n",
    "un_pdb_chain_dat = [f for f in init_df[\"pdb\"].unique().tolist() if \"AF-\" not in f]\n",
    "#print(len(un_pdb_chain_dat), un_pdb_chain_dat)\n",
    "\n",
    "download_pdbs_list(un_pdb_dat, path_to_dw_add_pdbs)\n",
    "\n",
    "df_chains = pdbs_to_chains(path_to_dw_add_pdbs, path_to_chains)\n",
    "filter_chains(path_to_chains, path_to_pdb_chains_filtered)\n",
    "rem_hetatm(path_to_pdb_chains_filtered, path_to_save_rem_hetatm)\n",
    "\n",
    "\n",
    "relaxed_chains = set(os.listdir(path_to_relaxed_chains))\n",
    "non_relaxed_chains = list(set(un_pdb_chain_dat) - relaxed_chains)\n",
    "\n",
    "run_relax()\n",
    "\n",
    "\n",
    "make_chain_dirs(path_to_relaxed_chains, un_pdb_chain_dat)\n",
    "copy_relaxed(path_to_ds)\n",
    "ds_mut = create_mut_df()\n",
    "\n",
    "run_rosetta_for_mutants()\n",
    "copy_pdb_files_and_directories([path_to_relaxed_chains, rosetta_out], relaxed_chains_total)\n",
    "feature_calc_nonori(relaxed_chains_total,features_ds_nonori)\n",
    "orient_dataset(relaxed_chains_total, relaxed_chains_total_ori)\n",
    "feature_calc(relaxed_chains_total_ori,features_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd51d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### specify here\n",
    "feature_type = \"defdif\"\n",
    "# evaluation_dataset_path = '/path_to_your_csv_dataset/' # NOTE: columns in dataset - \"pdb_id\",\"position\",\"wild_type\",\"mutant\",\"ddg\"\n",
    "evaluation_dataset_path = \"datasets/Ssym.csv\"\n",
    "\n",
    "#NOTE: here dataset should have the following column names \"pdb_id\",\"position\",\"wild_type\",\"mutant\",\"ddg\"\n",
    "#renaming example\n",
    "#init_df2 = pd.read_csv(\"ThermoNet/data/datasets/p53.txt\", sep = \" \", names=[\"pdb_id\",\"position\",\"wild_type\",\"mutant\",\"ddg\"])\n",
    "#init_df2.to_csv(\"datasets/p53_dataset.csv\")\n",
    "############################################\n",
    "\n",
    "evaluation_features_dir_dir = features_ds+f\"/{dataset_name}_{feature_type}_direct/\"\n",
    "evaluation_features_dir_rev = features_ds+f\"/{dataset_name}_{feature_type}_reverse/\"\n",
    "\n",
    "save_dataset_npy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367c30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
