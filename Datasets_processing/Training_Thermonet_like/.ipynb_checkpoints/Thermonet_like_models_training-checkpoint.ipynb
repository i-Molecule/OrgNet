{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5370c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Q1744\"\n",
    "feature_type = \"defdif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original thermonet like\n",
    "#training data\n",
    "training_features_dir_dir = f\"path_to_features/{dataset}_features/{dataset}_{feature_type}_direct/\"\n",
    "training_features_dir_rev = f\"path_to_features/{dataset}_features/{dataset}_{feature_type}_reverse/\"\n",
    "training_dataset_path = f\"path_to_datasets/datasets/{dataset}_direct.csv\"\n",
    "AUGMENT_REVERSE_MUTATIONS = True\n",
    "\n",
    "#evaluation data\n",
    "evaluation_features_dir_dir = f\"path_to_features/Ssym_{feature_type}_direct/\"\n",
    "evaluation_features_dir_rev = f\"path_to_features/Ssym_{feature_type}_reverse/\"\n",
    "evaluation_dataset_path = \"path_to_datasets/Ssym.csv\"\n",
    "\n",
    "#paths to save models and pictures\n",
    "#mp = f\"/home/nata/work/Projects/Protein_stability_prediction/Thermonet_var/14656_Unique_Mutations_Voxel_Features_PDBs/Models_default_TH/TH_{feature_type}_{dataset}/\"\n",
    "#picspathsave = f\"/home/nata/work/Projects/Protein_stability_prediction/Thermonet_var/14656_Unique_Mutations_Voxel_Features_PDBs/Models_default_TH/pics_loss_TH_{feature_type}_{dataset}/\"\n",
    "#evalpathsave = f\"/home/nata/work/Projects/Protein_stability_prediction/Thermonet_var/14656_Unique_Mutations_Voxel_Features_PDBs/Models_default_TH/evaluation_TH_{feature_type}_{dataset}/\"\n",
    "\n",
    "#for f in [mp, picspathsave, evalpathsave]:\n",
    "#    if os.path.exists(f) == False:\n",
    "#        os.mkdir(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oriented thermonet like\n",
    "#training data\n",
    "training_features_dir_dir = f\"path_to_oriented_features/{dataset}_{feature_type}_direct/\"\n",
    "training_features_dir_rev = f\"path_to_oriented_features/{dataset}_{feature_type}_reverse/\"\n",
    "training_dataset_path = f\"path_to_datasets/{dataset}_direct.csv\"\n",
    "AUGMENT_REVERSE_MUTATIONS = True\n",
    "\n",
    "#evaluation data\n",
    "evaluation_features_dir_dir = f\"path_to_oriented_features/Ssym_oriented_features/Ssym_{feature_type}_direct/\"\n",
    "evaluation_features_dir_rev = f\"path_to_oriented_features/Ssym_oriented_features/Ssym_{feature_type}_reverse/\"\n",
    "evaluation_dataset_path = \"path_to_datasets/Ssym.csv\"\n",
    "\n",
    "#paths to save models and pictures\n",
    "mp = f\"path_to_save_models/TH_{feature_type}_{dataset}/\"\n",
    "picspathsave = f\"path_to_save_pictures/pics_loss_TH_{feature_type}_{dataset}/\"\n",
    "evalpathsave = f\"path_to_save_csvs/evaluation_TH_{feature_type}_{dataset}/\"\n",
    "\n",
    "for f in [mp, picspathsave, evalpathsave]:\n",
    "    if os.path.exists(f) == False:\n",
    "        os.mkdir(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_training_set():\n",
    "    \n",
    "    print(\"1. Loading csv datasets\")\n",
    "    df = pd.read_csv(training_dataset_path)\n",
    "    print(f'Total unique mutations: {len(df)}')\n",
    "\n",
    "    #load direct features\n",
    "    df['features'] = df.apply(lambda r: f'{training_features_dir_dir}/{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy', axis=1)\n",
    "    df = df[df.features.apply(lambda v: os.path.exists(v))]\n",
    "    print(f'Total mutations with features: {len(df)}')\n",
    "    df.features = [np.load(f) for f in tqdm(df.features, desc=\"2. Loading features\")]\n",
    "    print(f'Total mutations after filtering: {len(df)}')\n",
    "    df_train = df\n",
    "    \n",
    "    if AUGMENT_REVERSE_MUTATIONS:\n",
    "        \n",
    "        print('Augmenting reverse mutations')\n",
    "        df_rev = pd.read_csv(training_dataset_path)\n",
    "        df_rev.ddg = -df_rev.ddg\n",
    "\n",
    "        \n",
    "        df_rev['features'] = df_rev.apply(lambda r: f'{training_features_dir_rev}/{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy', axis=1)\n",
    "        df_rev = df_rev[df_rev.features.apply(lambda v: os.path.exists(v))]\n",
    "        print(f'Total mutations with features: {len(df)}')\n",
    "        \n",
    "        \n",
    "        df_rev.features = [np.load(f) for f in tqdm(df_rev.features, desc=\"3. Loading features\")]\n",
    "        print(f'Total mutations after filtering: {len(df_rev)}')\n",
    "        df_train = pd.concat([df_train, df_rev], axis=0).sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "    df_train.features = df_train.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "def load_data_ssym_dir():\n",
    "    \n",
    "    print(\"Loading Ssym direct mutations\")\n",
    "    df = pd.read_csv(evaluation_dataset_path)\n",
    "    print(f'Total unique mutations: {len(df)}')\n",
    "\n",
    "    df['features'] = df.apply(lambda r: f'{evaluation_features_dir_dir}/{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy', axis=1)\n",
    "    df = df[df.features.apply(lambda v: os.path.exists(v))]\n",
    "    print(f'Total mutations with features: {len(df)}')\n",
    "    df.features = [np.load(f) for f in tqdm(df.features, desc=\"2. Loading features\")]\n",
    "    print(f'Total mutations after filtering: {len(df)}')\n",
    "\n",
    "    df_train = df\n",
    "    df_train.features = df_train.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "def load_data_ssym_rev():\n",
    "    \n",
    "    print('Loading Ssym reverse mutations')\n",
    "    df_rev = pd.read_csv(evaluation_dataset_path)\n",
    "    df_rev.ddg = -df_rev.ddg\n",
    "\n",
    "        \n",
    "    df_rev['features'] = df_rev.apply(lambda r: f'{evaluation_features_dir_rev}/{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy', axis=1)\n",
    "    df_rev = df_rev[df_rev.features.apply(lambda v: os.path.exists(v))]\n",
    "    print(f'Total mutations with features: {len(df_rev)}')\n",
    "        \n",
    "        \n",
    "    df_rev.features = [np.load(f) for f in tqdm(df_rev.features, desc=\"3. Loading features\")]\n",
    "    print(f'Total mutations after filtering: {len(df_rev)}')\n",
    "    \n",
    "    df_rev.features = df_rev.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    \n",
    "    return df_rev\n",
    "\n",
    "def build_model(model_type='regression', conv_layer_sizes=(16, 16, 16), dense_layer_size=16, dropout_rate=0.25):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # make sure requested model type is valid\n",
    "    if model_type not in ['regression', 'classification']:\n",
    "        print('Requested model type {0} is invalid'.format(model_type))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    # instantiate a 3D convnet\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv3D(filters=conv_layer_sizes[0], kernel_size=(3, 3, 3), input_shape=(16, 16, 16, 14)))\n",
    "    model.add(layers.Activation(activation='relu'))\n",
    "    model.add(layers.Conv3D(filters=conv_layer_sizes[1], kernel_size=(3, 3, 3)))\n",
    "    model.add(layers.Activation(activation='relu'))\n",
    "    model.add(layers.Conv3D(filters=conv_layer_sizes[2], kernel_size=(3, 3, 3)))\n",
    "    model.add(layers.Activation(activation='relu'))\n",
    "    model.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(units=dense_layer_size, activation='relu'))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # the last layer is dependent on model type\n",
    "    if model_type == 'regression':\n",
    "        model.add(layers.Dense(units=1))\n",
    "    else:\n",
    "        model.add(layers.Dense(units=3, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.0001),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def rmse(y_val_direct, y_pred):\n",
    "\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.squeeze(y_val_direct) - tf.squeeze(y_pred))))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "def pearson_r(y_val_direct, y_pred):\n",
    "\n",
    "    if tf.shape(y_val_direct)[0] == 1:\n",
    "        y_val_direct = tf.concat([y_val_direct, y_val_direct], axis=0)\n",
    "        y_pred = tf.concat([y_pred, y_pred], axis=0)\n",
    "\n",
    "        pr, _ = tf.py_function(sc.pearsonr, [y_val_direct, y_pred], [tf.float64, tf.float64])\n",
    "        #tf.print(\"Pearson correlation coefficient:\", pr)\n",
    "    else:\n",
    "        y_val_direct = tf.squeeze(y_val_direct)\n",
    "        y_pred = tf.squeeze(y_pred)\n",
    "    \n",
    "        pr, _ = tf.py_function(sc.pearsonr, [y_val_direct, y_pred], [tf.float64, tf.float64])\n",
    "        #tf.print(\"Pearson correlation coefficient:\", pr)\n",
    "\n",
    "    return pr\n",
    "\n",
    "class EvaluateAndStoreMetrics(Callback):\n",
    "    def __init__(self, X_val, y_val, key_prefix):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.key_prefix = key_prefix\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss, val_mae, val_mse, val_rmse, val_pearson_r = self.model.evaluate(\n",
    "            self.X_val, self.y_val, verbose=0\n",
    "        )\n",
    "\n",
    "        # Store the evaluation metrics in history.history\n",
    "        key_loss = self.key_prefix + 'loss'\n",
    "        key_mae = self.key_prefix + 'mae'\n",
    "        key_mse = self.key_prefix + 'mse'\n",
    "        key_rmse = self.key_prefix + 'rmse'\n",
    "        key_pearson_r = self.key_prefix + 'pearson_r'\n",
    "\n",
    "        logs[key_loss] = val_loss\n",
    "        logs[key_mae] = val_mae\n",
    "        logs[key_mse] = val_mse\n",
    "        logs[key_rmse] = val_rmse\n",
    "        logs[key_pearson_r] = val_pearson_r\n",
    "\n",
    "        # Print or log the metrics if needed\n",
    "        print(f\"\\nValidation Metrics after Epoch {epoch + 1}:\")\n",
    "        print(f\" - {key_loss}: {val_loss:.4f}\")\n",
    "        print(f\" - {key_mae}: {val_mae:.4f}\")\n",
    "        print(f\" - {key_mse}: {val_mse:.4f}\")\n",
    "        print(f\" - {key_rmse}: {val_rmse:.4f}\")\n",
    "        print(f\" - {key_pearson_r}: {val_pearson_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data_training_set()\n",
    "df_train_ssym_dir = load_data_ssym_dir()\n",
    "df_train_ssym_rev = load_data_ssym_rev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "conv_layer_sizes = (16, 24, 32)\n",
    "dense_layer_size = 24\n",
    "model = build_model('regression', conv_layer_sizes, dense_layer_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d13f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "X_direct = df_train.features.to_list()\n",
    "X_direct = np.array(X_direct)\n",
    "y_direct = df_train.ddg.to_numpy()\n",
    "sample_size = X_direct.shape[0]\n",
    "k = 10\n",
    "num_validation_samples = sample_size // k\n",
    "shp = df_train.shape[0]//10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ec420",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "ev = []\n",
    "log_loss_fold = []\n",
    "\n",
    "\n",
    "fold_num=0\n",
    "for i in (range(k)):\n",
    "    # create a validation set\n",
    "    X_val_direct = X_direct[i * num_validation_samples: (i + 1) * num_validation_samples]\n",
    "    y_val_direct = y_direct[i * num_validation_samples: (i + 1) * num_validation_samples]\n",
    "\n",
    "    # create the training data from all other partitions\n",
    "    X_train_direct = np.concatenate(\n",
    "        [X_direct[:i * num_validation_samples],\n",
    "         X_direct[(i + 1) * num_validation_samples:]],\n",
    "        axis=0\n",
    "    )\n",
    "    y_train_direct = np.concatenate(\n",
    "        [y_direct[:i * num_validation_samples],\n",
    "         y_direct[(i + 1) * num_validation_samples:]],\n",
    "        axis=0\n",
    "    )\n",
    "\n",
    "    # shuffle the training set\n",
    "    indices = np.arange(0, X_train_direct.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train_direct[indices]\n",
    "    y_train = y_train_direct[indices]\n",
    "\n",
    "\n",
    "    #prepare validation sets on Ssym direct dataset\n",
    "    X_direct_ssym_dir = np.array(df_train_ssym_dir.features.to_list())\n",
    "    y_direct_ssym_dir = df_train_ssym_dir.ddg.to_numpy()\n",
    "\n",
    "\n",
    "    #prepare validation sets on Ssym reverse dataset\n",
    "    X_direct_ssym_rev = np.array(df_train_ssym_rev.features.to_list())\n",
    "    y_direct_ssym_rev = df_train_ssym_rev.ddg.to_numpy()\n",
    "\n",
    "    \n",
    "    # checkpoint\n",
    "    model_path = mp+\"kerv1\" + '_member_' + str(i + 1) + '.h5'\n",
    "    m_nm = \"kerv1\" + '_member_' + str(i + 1) + '.h5'\n",
    "    checkpoint = callbacks.ModelCheckpoint(model_path, monitor='val_loss', verbose=1,\n",
    "                                           save_best_only=True, mode='min')\n",
    "    \n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    evaluate_callback_Ssym_dir = EvaluateAndStoreMetrics(X_direct_ssym_dir, y_direct_ssym_dir, key_prefix='val_direct_')\n",
    "\n",
    "    evaluate_callback_Ssym_rev = EvaluateAndStoreMetrics(X_direct_ssym_rev, y_direct_ssym_rev, key_prefix='val_rev_')\n",
    "    \n",
    "    callbacks_list = [checkpoint, early_stopping, evaluate_callback_Ssym_dir, evaluate_callback_Ssym_rev]\n",
    "\n",
    "    # build a model\n",
    "    model = build_model(model_type='regression', conv_layer_sizes=conv_layer_sizes,\n",
    "                        dense_layer_size=dense_layer_size, dropout_rate=0.5)\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        amsgrad=False\n",
    "        ),\n",
    "        metrics=['mae', 'mse', rmse, pearson_r]\n",
    "                 )\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train_direct, y_train_direct, validation_data=(X_val_direct, y_val_direct),\n",
    "                        epochs=100, batch_size=8, callbacks=callbacks_list, verbose=1)\n",
    "    result = {\n",
    "        'fold': fold_num,\n",
    "        'val_loss': min(history.history['val_loss']),\n",
    "        'val_mae': min(history.history['val_mae']),\n",
    "        'val_mse': min(history.history['val_mse']),\n",
    "        'val_rmse': min(history.history['val_rmse']),\n",
    "        'val_pearson_r': max(history.history['val_pearson_r'])\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    log_loss_fold.append(min(history.history['val_loss']))\n",
    "    fold_num += 1\n",
    "        \n",
    "    ev.append({'min_loss_CV': min(log_loss_fold), 'mean_loss_CV': np.mean(log_loss_fold)})\n",
    "    \n",
    "    df_results_inter = pd.DataFrame(results)\n",
    "    ev_results_inter =pd.DataFrame(ev)\n",
    "    ev_results_inter.to_csv(evalpathsave, index=False)\n",
    "    df_results_inter.to_csv(evalpathsave, index=False)\n",
    "    \n",
    "    \n",
    "    # Access the training history\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Subplot 1: Training and Validation Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{m_nm} Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Subplot 2: MSE, RMSE and Pearson_r\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['mse'], label='Training MSE')\n",
    "    plt.plot(epochs, history.history['rmse'], label='Training RMSE')\n",
    "    plt.plot(epochs, history.history['pearson_r'], label='Training Pearson_r')\n",
    "    plt.plot(epochs, history.history['val_mse'], label='Validation MSE')\n",
    "    plt.plot(epochs, history.history['val_rmse'], label='Validation RMSE')\n",
    "    plt.plot(epochs, history.history['val_pearson_r'], label='Validation Pearson_r')\n",
    "    plt.title(f'{m_nm} MSE, RMSE and Pearson_r on Training and Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{picspathsave}/{m_nm}_{feature_type}_losses_def.png\", dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting evaluation metrics for the second validation set\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.plot(epochs, history.history['val_direct_loss'], label='Validation Loss Ssym_dir')\n",
    "    plt.plot(epochs, history.history['val_direct_mae'], label='Validation MAE Ssym_dir')\n",
    "    plt.plot(epochs, history.history['val_direct_mse'], label='Validation MSE Ssym_dir')\n",
    "    plt.plot(epochs, history.history['val_direct_rmse'], label='Validation RMSE Ssym_dir')\n",
    "    plt.plot(epochs, history.history['val_direct_pearson_r'], label='Validation Pearson_r Ssym_dir')\n",
    "\n",
    "    plt.title(f'{m_nm} Evaluation Metrics for Ssym_dir')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{picspathsave}/{m_nm}_{feature_type}_ssym_dir_def.png\", dpi = 300)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting evaluation metrics for the third validation set\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.plot(epochs, history.history['val_rev_loss'], label='Validation Loss Ssym_rev')\n",
    "    plt.plot(epochs, history.history['val_rev_mae'], label='Validation MAE Ssym_rev')\n",
    "    plt.plot(epochs, history.history['val_rev_mse'], label='Validation MSE Ssym_rev')\n",
    "    plt.plot(epochs, history.history['val_rev_rmse'], label='Validation RMSE Ssym_rev')\n",
    "    plt.plot(epochs, history.history['val_rev_pearson_r'], label='Validation Pearson_r Ssym_rev')\n",
    "\n",
    "    plt.title(f'{m_nm} Evaluation Metrics for Ssym_rev')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{picspathsave}/{m_nm}_{feature_type}_ssym_rev_def.png\", dpi = 300)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
