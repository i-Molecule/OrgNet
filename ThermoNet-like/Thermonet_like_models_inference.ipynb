{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f92de8",
   "metadata": {},
   "source": [
    "# Load all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.stats as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "from argparse import ArgumentParser\n",
    "from keras.models import load_model\n",
    "from keras.saving import register_keras_serializable\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce0b9f",
   "metadata": {},
   "source": [
    "# Load inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248d3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dataset_dir(evaluation_dataset_path, evaluation_features_dir_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads data from the processed dataset, given the path to csv file and path to features. Note that dataset\n",
    "    is loaded into RAM, so be carefull with that.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      evaluation_dataset_path:\n",
    "          Path to dataset csv file. Note the file columns format.\n",
    "          \n",
    "      evaluation_features_dir_dir:\n",
    "          Path to direct features directory. Note the directory format.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Pandas dataframe with loaded .npy features.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading dataset direct mutations\")\n",
    "    df = pd.read_csv(evaluation_dataset_path)\n",
    "    print(f'Total unique mutations: {len(df)}')\n",
    "\n",
    "    df['features'] = df.apply(lambda r: f'{evaluation_features_dir_dir}/{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy', axis=1)\n",
    "    df = df[df.features.apply(lambda v: os.path.exists(v))]\n",
    "    print(f'Total mutations with features: {len(df)}')\n",
    "    df.features = [np.load(f) for f in tqdm(df.features, desc=\"2. Loading features\")]\n",
    "    print(f'Total mutations after filtering: {len(df)}')\n",
    "\n",
    "    df_train = df\n",
    "    df_train.features = df_train.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "def load_data_dataset_rev(evaluation_dataset_path, evaluation_features_dir_rev):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads data from the processed dataset, given the path to csv file and path to features. Note that dataset\n",
    "    is loaded into RAM, so be carefull with that.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      evaluation_dataset_path:\n",
    "          Path to dataset csv file. Note the file columns format.\n",
    "          \n",
    "      evaluation_features_dir_rev:\n",
    "          Path to reverse features directory. Note the directory format.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Pandas dataframe with loaded .npy features.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Loading dataset reverse mutations')\n",
    "    df_rev = pd.read_csv(evaluation_dataset_path)\n",
    "    df_rev.ddg = -df_rev.ddg\n",
    "\n",
    "        \n",
    "    df_rev['features'] = df_rev.apply(lambda r: f'{evaluation_features_dir_rev}/{r.pdb_id}/{r.pdb_id}_{r.wild_type}{r.position}{r.mutant}.npy', axis=1)\n",
    "    df_rev = df_rev[df_rev.features.apply(lambda v: os.path.exists(v))]\n",
    "    print(f'Total mutations with features: {len(df_rev)}')\n",
    "        \n",
    "        \n",
    "    df_rev.features = [np.load(f) for f in tqdm(df_rev.features, desc=\"3. Loading features\")]\n",
    "    print(f'Total mutations after filtering: {len(df_rev)}')\n",
    "    \n",
    "    df_rev.features = df_rev.features.apply(lambda k: np.transpose(k, (1, 2, 3, 0)))\n",
    "    \n",
    "    return df_rev\n",
    "\n",
    "@register_keras_serializable()\n",
    "def rmse(y_val_direct, y_pred):\n",
    "\n",
    "    rmse = tf.sqrt(tf.reduce_mean(tf.square(tf.squeeze(y_val_direct) - tf.squeeze(y_pred))))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# def pearson_r(y_val_direct, y_pred):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Simple function to calculate Pearson correlation coefficient, needed for model to load.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if tf.shape(y_val_direct)[0] == 1:\n",
    "#         y_val_direct = tf.concat([y_val_direct, y_val_direct], axis=0)\n",
    "#         y_pred = tf.concat([y_pred, y_pred], axis=0)\n",
    "\n",
    "#         pr, _ = tf.py_function(sc.pearsonr, [y_val_direct, y_pred], [tf.float64, tf.float64])\n",
    "#         #tf.print(\"Pearson correlation coefficient:\", pr)\n",
    "#     else:\n",
    "#         y_val_direct = tf.squeeze(y_val_direct)\n",
    "#         y_pred = tf.squeeze(y_pred)\n",
    "    \n",
    "#         pr, _ = tf.py_function(sc.pearsonr, [y_val_direct, y_pred], [tf.float64, tf.float64])\n",
    "#         #tf.print(\"Pearson correlation coefficient:\", pr)\n",
    "\n",
    "#     return pr\n",
    "\n",
    "@register_keras_serializable()\n",
    "def pearson_r(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation coefficient as a Keras metric.\n",
    "    \"\"\"\n",
    "    # Ensure tensors are flattened\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "    # Compute mean and standard deviation\n",
    "    mean_y_true = tf.reduce_mean(y_true)\n",
    "    mean_y_pred = tf.reduce_mean(y_pred)\n",
    "    std_y_true = tf.math.reduce_std(y_true)\n",
    "    std_y_pred = tf.math.reduce_std(y_pred)\n",
    "\n",
    "    # Compute covariance\n",
    "    covariance = tf.reduce_mean((y_true - mean_y_true) * (y_pred - mean_y_pred))\n",
    "\n",
    "    # Compute Pearson correlation coefficient\n",
    "    pearson_corr = covariance / (std_y_true * std_y_pred + tf.keras.backend.epsilon())\n",
    "\n",
    "    return pearson_corr\n",
    "\n",
    "# Function to prepare datasets\n",
    "def prepare_datasets(df_train_dataset_dir, df_train_dataset_rev):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple function to prepare the datasets.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      df_train_dataset_dir:\n",
    "          Dataframe with loaded direct features can be obtained via load_data_dataset_dir().\n",
    "          \n",
    "      df_train_dataset_rev:\n",
    "          Dataframe with loaded reverse features can be obtained via load_data_dataset_rev().\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Collection of datasets in numpy format used for inference: direct, reverse and total.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_direct_dataset_dir = np.array(df_train_dataset_dir.features.to_list())\n",
    "    y_direct_dataset_dir = df_train_dataset_dir.ddg.to_numpy()\n",
    "\n",
    "    X_direct_dataset_rev = np.array(df_train_dataset_rev.features.to_list())\n",
    "    y_direct_dataset_rev = df_train_dataset_rev.ddg.to_numpy()\n",
    "    \n",
    "    g = pd.concat([df_train_dataset_dir, df_train_dataset_rev])\n",
    "    X_total_dataset = np.array(g.features.to_list())\n",
    "    y_total_dataset = g.ddg.to_numpy()\n",
    "\n",
    "    return X_direct_dataset_dir, y_direct_dataset_dir,X_direct_dataset_rev, y_direct_dataset_rev, X_total_dataset, y_total_dataset\n",
    "\n",
    "# Function to evaluate a single model\n",
    "def evaluate_model(model_path, X_dir, y_dir, X_rev, y_rev , X_tot, y_tot, model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple function to for an inference of single model.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      model_path:\n",
    "          Path to models.\n",
    "          \n",
    "      X_dir:\n",
    "          Direct dataset in numpy format.\n",
    "        \n",
    "      y_dir:\n",
    "          Direct dataset labels in numpy format.\n",
    "          \n",
    "      X_rev:\n",
    "          Reverse dataset in numpy format.\n",
    "        \n",
    "      y_rev:\n",
    "          Reverse dataset labels in numpy format..\n",
    "          \n",
    "      X_tot:\n",
    "          Total dataset in numpy format.\n",
    "      \n",
    "      y_tot:\n",
    "          Total dataset labels in numpy format.\n",
    "          \n",
    "      model_name:\n",
    "          Specify model name.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Dictionary with inference metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = load_model(model_path, custom_objects={\"mse\": MeanSquaredError(), \"rmse\": rmse, \"pearson_r\": pearson_r})\n",
    "    \n",
    "    y_pred_dir = model.predict(X_dir).reshape(-1)\n",
    "    y_pred_rev = model.predict(X_rev).reshape(-1)\n",
    "    y_pred_tot = model.predict(X_tot).reshape(-1)\n",
    "\n",
    "    mae_dir = mean_absolute_error(y_dir, y_pred_dir)\n",
    "    mse_dir = mean_squared_error(y_dir, y_pred_dir)\n",
    "    # rmse_dir = mean_squared_error(y_dir, y_pred_dir, squared=False)\n",
    "    rmse_dir = root_mean_squared_error(y_dir, y_pred_dir)\n",
    "    pr_dir = sc.pearsonr(y_dir, y_pred_dir)[0]\n",
    "\n",
    "    mae_rev = mean_absolute_error(y_rev, y_pred_rev)\n",
    "    mse_rev = mean_squared_error(y_rev, y_pred_rev)\n",
    "    # rmse_rev = mean_squared_error(y_rev, y_pred_rev, squared=False)\n",
    "    rmse_rev = root_mean_squared_error(y_rev, y_pred_rev)\n",
    "    pr_rev = sc.pearsonr(y_rev, y_pred_rev)[0]\n",
    "    \n",
    "    mae_tot = mean_absolute_error(y_tot, y_pred_tot)\n",
    "    mse_tot = mean_squared_error(y_tot, y_pred_tot)\n",
    "    # rmse_tot = mean_squared_error(y_tot, y_pred_tot, squared=False)\n",
    "    rmse_tot = root_mean_squared_error(y_tot, y_pred_tot)\n",
    "    pr_tot = sc.pearsonr(y_tot, y_pred_tot)[0]\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"mae_dir\": mae_dir, \"mse_dir\": mse_dir, \"rmse_dir\": rmse_dir, \"pearson_r_dir\": pr_dir,\n",
    "        \"mae_rev\": mae_rev, \"mse_rev\": mse_rev, \"rmse_rev\": rmse_rev, \"pearson_r_rev\": pr_rev,\n",
    "        \"mae_tot\": mae_tot, \"mse_tot\": mse_tot, \"rmse_tot\": rmse_tot, \"pearson_r_tot\": pr_tot\n",
    "    }\n",
    "\n",
    "# Function to evaluate all models\n",
    "def evaluate_models(df_train_dataset_dir, df_train_dataset_rev, model_dir, evalpathsave, model_type, v):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple function to for an inference of several model.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      df_train_dataset_dir:\n",
    "          Dataframe with loaded direct features can be obtained via load_data_dataset_dir().\n",
    "          \n",
    "      df_train_dataset_rev:\n",
    "          Dataframe with loaded reverse features can be obtained via load_data_dataset_dir().\n",
    "        \n",
    "      model_dir:\n",
    "          Path to directory with models.\n",
    "          \n",
    "      evalpathsave:\n",
    "          Path to save inference Dataframe.\n",
    "        \n",
    "      model_type:\n",
    "          Selection of a model type \"sing\" or \"ens\".\n",
    "          \n",
    "      v:\n",
    "          Flag.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Inference dataframe with metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_dir, y_dir, X_rev, y_rev, X_tot, y_tot = prepare_datasets(df_train_dataset_dir, df_train_dataset_rev)\n",
    "    \n",
    "    eval_results = []\n",
    "    \n",
    "    if model_type == \"sing\":\n",
    "        for model_name in sorted(os.listdir(model_dir), key=lambda x: int(x.split(\".\")[0].split(\"_\")[-1])):\n",
    "        \n",
    "            model_path = os.path.join(model_dir, model_name)\n",
    "            results = evaluate_model(model_path, X_dir, y_dir, X_rev, y_rev , X_tot, y_tot, model_name)\n",
    "            eval_results.append(results)\n",
    "    \n",
    "    if model_type == \"ens\":\n",
    "        results = evaluate_ensemble(model_dir, X_dir, y_dir, X_rev, y_rev , X_tot, y_tot)\n",
    "        eval_results.append(results)\n",
    "\n",
    "    eval_df = pd.DataFrame(eval_results)\n",
    "    eval_df.to_csv(f\"{evalpathsave}/{model_dir.split('/')[-2]}_dataset_eval_results_{v}.csv\", index=False)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "def evaluate_ensemble(models_dir, X_dir, y_dir, X_rev, y_rev, X_tot, y_tot):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simple function to for an inference of an ensemble.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      models_dir:\n",
    "          Path to models directory.\n",
    "          \n",
    "      X_dir:\n",
    "          Direct dataset in numpy format.\n",
    "        \n",
    "      y_dir:\n",
    "          Direct dataset labels in numpy format.\n",
    "          \n",
    "      X_rev:\n",
    "          Reverse dataset in numpy format.\n",
    "        \n",
    "      y_rev:\n",
    "          Reverse dataset labels in numpy format..\n",
    "          \n",
    "      X_tot:\n",
    "          Total dataset in numpy format.\n",
    "      \n",
    "      y_tot:\n",
    "          Total dataset labels in numpy format.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Dictionary with ensemble metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_files = sorted(os.listdir(models_dir), key=lambda x: int(x.split(\".\")[0].split(\"_\")[-1]))\n",
    "    ensemble_preds_dir = []\n",
    "    ensemble_preds_rev = []\n",
    "    ensemble_preds_tot = []\n",
    "\n",
    "    for model_name in model_files:\n",
    "        model_path = os.path.join(models_dir, model_name)\n",
    "        model = load_model(model_path, custom_objects={\"mse\": MeanSquaredError(), \"rmse\": rmse, \"pearson_r\": pearson_r})\n",
    "\n",
    "        y_pred_dir = model.predict(X_dir).reshape(-1)\n",
    "        y_pred_rev = model.predict(X_rev).reshape(-1)\n",
    "        y_pred_tot = model.predict(X_tot).reshape(-1)\n",
    "\n",
    "        ensemble_preds_dir.append(y_pred_dir)\n",
    "        ensemble_preds_rev.append(y_pred_rev)\n",
    "        ensemble_preds_tot.append(y_pred_tot)\n",
    "\n",
    "    # Average predictions across all models\n",
    "    avg_pred_dir = np.mean(ensemble_preds_dir, axis=0)\n",
    "    avg_pred_rev = np.mean(ensemble_preds_rev, axis=0)\n",
    "    avg_pred_tot = np.mean(ensemble_preds_tot, axis=0)\n",
    "\n",
    "    # Evaluate ensemble predictions\n",
    "    mae_dir = mean_absolute_error(y_dir, avg_pred_dir)\n",
    "    mse_dir = mean_squared_error(y_dir, avg_pred_dir)\n",
    "    # rmse_dir = mean_squared_error(y_dir, avg_pred_dir, squared=False)\n",
    "    rmse_dir = root_mean_squared_error(y_dir, avg_pred_dir)\n",
    "    pr_dir = sc.pearsonr(y_dir, avg_pred_dir)[0]\n",
    "\n",
    "    mae_rev = mean_absolute_error(y_rev, avg_pred_rev)\n",
    "    mse_rev = mean_squared_error(y_rev, avg_pred_rev)\n",
    "    # rmse_rev = mean_squared_error(y_rev, avg_pred_rev, squared=False)\n",
    "    rmse_rev = root_mean_squared_error(y_rev, avg_pred_rev)\n",
    "    pr_rev = sc.pearsonr(y_rev, avg_pred_rev)[0]\n",
    "    \n",
    "    \n",
    "    mae_tot = mean_absolute_error(y_tot, avg_pred_tot)\n",
    "    mse_tot = mean_squared_error(y_tot, avg_pred_tot)\n",
    "    # rmse_tot = mean_squared_error(y_tot, avg_pred_tot, squared=False)\n",
    "    rmse_tot = root_mean_squared_error(y_tot, avg_pred_tot)\n",
    "    pr_tot = sc.pearsonr(y_tot, avg_pred_tot)[0]\n",
    "\n",
    "    return {\n",
    "        \"mae_dir\": mae_dir, \"mse_dir\": mse_dir, \"rmse_dir\": rmse_dir, \"pearson_r_dir\": pr_dir,\n",
    "        \"mae_rev\": mae_rev, \"mse_rev\": mse_rev, \"rmse_rev\": rmse_rev, \"pearson_r_rev\": pr_rev,\n",
    "        \"mae_tot\": mae_tot, \"mse_tot\": mse_tot, \"rmse_tot\": rmse_tot, \"pearson_r_tot\": pr_tot\n",
    "    }\n",
    "\n",
    "def add_model_col(df, key):\n",
    "    df['model'] = [key+\"_\"+f.split(\".\")[0].split('_')[-1] for f in df['model_name']]\n",
    "    return df\n",
    "def add_model_col_ens(df, key):\n",
    "    df['model'] = key\n",
    "    return df\n",
    "\n",
    "def table_report(evalpathsave, model_dir, w, df_train_dataset_dir, df_train_dataset_rev):\n",
    "\n",
    "    \"\"\"\n",
    "    General function for OrgNet inference.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "      evalpathsave:\n",
    "          Path to save iference dataframes.\n",
    "          \n",
    "      model_dir:\n",
    "          Path to OrgNet models.\n",
    "        \n",
    "      w:\n",
    "          flag.\n",
    "          \n",
    "      df_train_dataset_dir:\n",
    "          Reverse dataset in numpy format.\n",
    "        \n",
    "      df_train_dataset_rev:\n",
    "          Reverse dataset labels in numpy format.\n",
    "          \n",
    "    Returns:\n",
    "    \n",
    "      Inference dataframe with metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    model_type = \"sing\"\n",
    "    key = f\"{w}_{model_type}\"\n",
    "    df1 = evaluate_models(df_train_dataset_dir, df_train_dataset_rev, model_dir, evalpathsave, model_type, key)\n",
    "    df1 = add_model_col(df1, key)\n",
    "    \n",
    "    model_type = \"ens\"\n",
    "    key = f\"{w}_{model_type}\"\n",
    "    df2 = evaluate_models(df_train_dataset_dir, df_train_dataset_rev, model_dir, evalpathsave, model_type, key)\n",
    "    df2 = add_model_col_ens(df2, key)\n",
    "    \n",
    "    df = pd.concat([df1, df2])\n",
    "    return df\n",
    "\n",
    "params = [\"model\", \"mae_dir\", \"mse_dir\",\"rmse_dir\", \"pearson_r_dir\",\"mae_rev\",\"mse_rev\", \"rmse_rev\",\"pearson_r_rev\", \"mae_tot\", \"mse_tot\",\"rmse_tot\",\"pearson_r_tot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93dac5",
   "metadata": {},
   "source": [
    "# Run the cell bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8857173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your paths\n",
    "\n",
    "feature_type = \"defdif\"\n",
    "# feature_type = \"def\"\n",
    "\n",
    "datasets_dir = \".../data_preprocessing/datasets\"\n",
    "features_dir = \".../data_preprocessing/Ssym/features/Ssym_ori\"\n",
    "# features_dir = \".../data_preprocessing/Ssym/features/Ssym_nonori\"\n",
    "inference_features_dir_dir =  f\"{features_dir}/Ssym_{feature_type}_direct/\"\n",
    "inference_features_dir_rev =  f\"{features_dir}/Ssym_{feature_type}_reverse/\"\n",
    "inference_dataset_path = f\"{datasets_dir}/Ssym.csv\"\n",
    "\n",
    "#load your data\n",
    "df_train_dataset_dir = load_data_dataset_dir(inference_dataset_path, inference_features_dir_dir)\n",
    "df_train_dataset_rev = load_data_dataset_rev(inference_dataset_path, inference_features_dir_rev)\n",
    "\n",
    "#get inference df\n",
    "# evalpathsave = \"/evalpathsave/\"\n",
    "evalpathsave = \".../ThermoNet-like/\"\n",
    "model_dir = f\".../ThermoNet-like/TH_{feature_type}_Q1744/\"\n",
    "w = f\"fflagff\"\n",
    "res_df = table_report(evalpathsave, model_dir, w, df_train_dataset_dir, df_train_dataset_rev)\n",
    "res_df[params]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-v5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
